{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='content'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> \n",
    "\n",
    "**Содержание**:\n",
    "\n",
    "* <a href='#know_the_data'>Загрузка данных</a> </br>          \n",
    "* <a href='#data_preprocessing'>Предобработка данных</a> </br>     \n",
    "* <a href='#model_training'>Обучение модели</a> </br>                    \n",
    "* <a href='#testing'>Тестирование</a></br>                        \n",
    "* <a href='#conclusions'>Выводы</a></br>                          \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='know_the_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn==1.2.2 -q matplotlib==3.9.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==2.2.2 seaborn==0.13.2 numba==0.60.0 numpy==1.26.4  -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymystem3 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/irinamistulova/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/irinamistulova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import warnings\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import notebook, tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# Загрузка ресурсов NLTK (один раз)\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Игнорирование предупреждений\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "1           1   \n",
       "2           2   \n",
       "3           3   \n",
       "4           4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                           Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                           Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "        df = pd.read_csv('../assets/toxic_comments.csv') \n",
    "except:\n",
    "        df = pd.read_csv('/datasets/toxic_comments.csv') \n",
    "    \n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_be12c_row0_col0, #T_be12c_row1_col0, #T_be12c_row2_col0 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_be12c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_be12c_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_be12c_level0_row0\" class=\"row_heading level0 row0\" >Unnamed: 0</th>\n",
       "      <td id=\"T_be12c_row0_col0\" class=\"data row0 col0\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be12c_level0_row1\" class=\"row_heading level0 row1\" >text</th>\n",
       "      <td id=\"T_be12c_row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be12c_level0_row2\" class=\"row_heading level0 row2\" >toxic</th>\n",
       "      <td id=\"T_be12c_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2810871d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(df.isna().mean()*100).round(2)\n",
    "            .style.background_gradient('coolwarm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6,5))  # Use 'ax' instead of 'axes'\n",
    "sns.histplot(df, x='toxic', ax=ax)  # Use 'ax' here as well\n",
    "ax.set_title('Распределение по целевому признаку')\n",
    "ax.set_ylabel('Количество')\n",
    "ax.set_xlabel('Токсичность комментария (да - 1, нет - 0)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 159448, 159449, 159450])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Unnamed: 0'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                           Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                           Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.drop(columns= ['Unnamed: 0'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы**\n",
    "\n",
    "- Пропуски отсутсвуют\n",
    "- Столбец дублирующий интексы удален\n",
    "- Текс на английском языке, будет использован словарь стоп слов на английском\n",
    "- Целевой признак имеет дисбаланс классов. Поэтому целесообразнее для оценки модели использовать метрику F1-мера: среднее гармоническое между precision и recall. Также для устранения дибаланса будет использован Undersampling (уменьшение выборки), т.к. будет использоваться кросс-валидация и оверсемплинг не подойдет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#content'>Вернуться к оглавлению</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_preprocessing'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Лемматизируем твиты из колонки text, \n",
    "- удалим из предложений стоп слова, которые не несут важной информации для прогнозирования. \n",
    "- удалим знаки припинания и приведем все к нижнему регистру, для того, чтобы уменьшить объем матрицы, при переводе текста в векторный вид."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You, sir, are my hero. Any chance you remember what page that's on?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df['text'][4]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you  sir  are my hero  any chance you remember what page that s on '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_re = re.sub(r'[^a-zA-Z\\s]+', ' ', test.lower())\n",
    "test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 'PRP'),\n",
       " ('sir', 'VBP'),\n",
       " ('are', 'VBP'),\n",
       " ('my', 'PRP$'),\n",
       " ('hero', 'NN'),\n",
       " ('any', 'DT'),\n",
       " ('chance', 'NN'),\n",
       " ('you', 'PRP'),\n",
       " ('remember', 'VBP'),\n",
       " ('what', 'WP'),\n",
       " ('page', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('s', 'VBZ'),\n",
       " ('on', 'IN')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tags = nltk.pos_tag(test_re.split())\n",
    "test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet', quiet=True)  \n",
    "nltk.download('stopwords', quiet=True)  \n",
    "nltk.download('averaged_perceptron_tagger', quiet=True) \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(nltk_stopwords.words('english'))\n",
    "\n",
    "def lemmatize_and_remove_stopwords(text):\n",
    "   \n",
    "    text = re.sub(r'[^a-zA-Z\\s]+', ' ', text.lower())\n",
    "    words_with_tags = nltk.pos_tag(text.split())  # POS tags\n",
    "    lemmatized_words = []\n",
    "\n",
    "    for word, tag in words_with_tags:\n",
    "        if word not in stop_words:\n",
    "            if tag.startswith('J'):\n",
    "                pos = 'a'  # Adjective\n",
    "            elif tag.startswith('V'):\n",
    "                pos = 'v'  # Verb\n",
    "            elif tag.startswith('N'):\n",
    "                pos = 'n'  # Noun\n",
    "            elif tag.startswith('R'):\n",
    "                pos = 'r'  # Adverb\n",
    "            else:\n",
    "                pos = None  # Use default WordNet lemmatization (noun)\n",
    "\n",
    "            lemma = lemmatizer.lemmatize(word, pos=pos) if pos else lemmatizer.lemmatize(word)\n",
    "            lemmatized_words.append(lemma)\n",
    "\n",
    "    return \" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [04:24<00:00, 603.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation edits make username hardcore metallica fan revert vandalism closure gas vote new york doll fac please remove template talk page since retire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "      <td>aww match background colour seemingly stick thanks talk january utc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man really try edit war guy constantly remove relevant information talk edits instead talk page seem care formatting actual info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>make real suggestion improvement wonder section statistic later subsection type accident think reference may need tidy exact format ie date format etc later one else first preference format style reference want please let know appear backlog article review guess may delay reviewer turn list relevant form eg wikipedia good article nomination transport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the tools well.  · talk \"</td>\n",
       "      <td>0</td>\n",
       "      <td>congratulation well use tool well talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>cocksucker piss around work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.</td>\n",
       "      <td>0</td>\n",
       "      <td>vandalism matt shirvington article revert please ban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169</td>\n",
       "      <td>0</td>\n",
       "      <td>sorry word nonsense offensive anyway intend write anything article wow would jump vandalism merely request encyclopedic one use school reference selective breeding page almost stub point animal breeding short messy article give info must someone around expertise eugenics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contrary to those of DuLithgow</td>\n",
       "      <td>0</td>\n",
       "      <td>alignment subject contrary dulithgow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                           Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                           Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \"\\n\\nCongratulations from me as well, use the tools well.  · talk \"   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.   \n",
       "8                                                                                                                                                            Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              alignment on this subject and which are contrary to those of DuLithgow   \n",
       "\n",
       "   toxic  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "5      0   \n",
       "6      1   \n",
       "7      0   \n",
       "8      0   \n",
       "9      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                    lemmatized_text  \n",
       "0                                                                                                                                                                                                          explanation edits make username hardcore metallica fan revert vandalism closure gas vote new york doll fac please remove template talk page since retire  \n",
       "1                                                                                                                                                                                                                                                                                               aww match background colour seemingly stick thanks talk january utc  \n",
       "2                                                                                                                                                                                                                              hey man really try edit war guy constantly remove relevant information talk edits instead talk page seem care formatting actual info  \n",
       "3  make real suggestion improvement wonder section statistic later subsection type accident think reference may need tidy exact format ie date format etc later one else first preference format style reference want please let know appear backlog article review guess may delay reviewer turn list relevant form eg wikipedia good article nomination transport  \n",
       "4                                                                                                                                                                                                                                                                                                                                     sir hero chance remember page  \n",
       "5                                                                                                                                                                                                                                                                                                                            congratulation well use tool well talk  \n",
       "6                                                                                                                                                                                                                                                                                                                                       cocksucker piss around work  \n",
       "7                                                                                                                                                                                                                                                                                                              vandalism matt shirvington article revert please ban  \n",
       "8                                                                                   sorry word nonsense offensive anyway intend write anything article wow would jump vandalism merely request encyclopedic one use school reference selective breeding page almost stub point animal breeding short messy article give info must someone around expertise eugenics  \n",
       "9                                                                                                                                                                                                                                                                                                                              alignment subject contrary dulithgow  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['lemmatized_text'] = df['text'].progress_apply(lemmatize_and_remove_stopwords)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим тексты на наличие дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер до удаления дубликатов: (159292, 3)\n",
      "Размер после удаления дубликатов: (157364, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Размер до удаления дубликатов:', df.shape)\n",
    "df = df.drop_duplicates(keep = 'first', subset='lemmatized_text')\n",
    "print('Размер после удаления дубликатов:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 157364 entries, 0 to 157363\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   text             157364 non-null  object\n",
      " 1   toxic            157364 non-null  int64 \n",
      " 2   lemmatized_text  157364 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст необходимо представить в векторном виде. Для этого будем использовать \n",
    "TF-IDF (учет контектса частичный, за счет учета редкости слова, различает важность слов, требования в вычислительной мощности выше, чем у мешка слов, но ниже чем у BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы**\n",
    "\n",
    "- Тексты лемматизированы\n",
    "- Удалены стоп-слова\n",
    "- Удалены дубликаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#content'>Вернуться к оглавлению</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_training'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы решаем задачу классификации с учителем. Будем обучать модель логистической регрессии на двух подготовленных входных признаках. В качестве оценки будем использовать F1-меру - это  гармоническое среднее между precision и recall. F1-мера стремится к максимуму, когда precision и recall сбалансированы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['lemmatized_text']\n",
    "y= df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                    y, \n",
    "                                    test_size=0.25, \n",
    "                                    random_state=42)\n",
    "corpus = list(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы X_train: (118023, 132000)\n",
      "Размер матрицы X_test: (39341, 132000)\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vector =TfidfVectorizer() \n",
    "X_train = tf_idf_vector.fit_transform(corpus)\n",
    "X_test = tf_idf_vector.transform(X_test)\n",
    "\n",
    "print(\"Размер матрицы X_train:\", X_train.shape)\n",
    "print(\"Размер матрицы X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['carinatus', 'caring', 'carington', 'carioca', 'carion',\n",
       "       'carisbrook', 'caristii', 'carit', 'carjacker', 'carl', 'carla',\n",
       "       'carlaude', 'carle', 'carlebach', 'carles', 'carleton',\n",
       "       'carletons', 'carlie', 'carlile', 'carlin', 'carling',\n",
       "       'carlingford', 'carlinhos', 'carlisle', 'carlito', 'carlmarche',\n",
       "       'carlo', 'carload', 'carlos', 'carlossuarez', 'carlotta',\n",
       "       'carlsbad', 'carlsberg', 'carlsen', 'carlson', 'carlsruhe',\n",
       "       'carlsson', 'carlton', 'carly', 'carlyn', 'carma', 'carmack',\n",
       "       'carmaker', 'carman', 'carmarthen', 'carmarthenshire',\n",
       "       'carmegenon', 'carmel', 'carmelite', 'carmen'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tf_idf_vector.get_feature_names_out()\n",
    "feature_names[18000:18050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28586946, 0.29821648, 0.19951923, ..., 0.08646987, 0.10543201,\n",
       "       0.09145243])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация не требуется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Пайплайн с выбором модели через GridSearch\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', LogisticRegression())  # Плейсхолдер для модели\n",
    "])\n",
    "\n",
    "# Параметры для RandomizedSearch\n",
    "parameters = [\n",
    "    {  # Для LogisticRegression\n",
    "        'classifier': [LogisticRegression(random_state=42, max_iter=5000)],\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__penalty': ['l1', 'l2']\n",
    "    },\n",
    "     {  # Для SGDClassifier\n",
    "        'classifier': [SGDClassifier(class_weight='balanced', \n",
    "                                     max_iter=5000, \n",
    "                                     random_state=42)],\n",
    "        'classifier__loss': ['hinge'],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "        'classifier__learning_rate': ['constant', 'optimal'],\n",
    "        'classifier__eta0': [0.001, 0.01]\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "# Настройка RandomizedSearch\n",
    "rs = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    parameters,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.01, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   0.8s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.01, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   0.9s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.01, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   0.9s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.01, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   0.9s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.01, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   1.8s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   2.3s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   2.6s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   2.6s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   1.8s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   0.6s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   0.7s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   2.9s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   0.6s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   0.7s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   0.7s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   3.7s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   3.8s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   1.4s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   3.4s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   1.6s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   3.9s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   1.7s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   1.5s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   0.6s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   0.6s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   1.0s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   0.7s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   3.9s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   1.9s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   0.7s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   3.9s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   4.0s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   3.9s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   4.2s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   4.1s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   4.5s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   4.6s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   4.3s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=0.1, classifier__penalty=l1; total time=   0.0s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=0.1, classifier__penalty=l1; total time=   0.0s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=0.1, classifier__penalty=l1; total time=   0.1s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=0.1, classifier__penalty=l1; total time=   0.0s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=0.1, classifier__penalty=l1; total time=   0.0s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=10, classifier__penalty=l1; total time=   0.0s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=10, classifier__penalty=l1; total time=   0.0s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=10, classifier__penalty=l1; total time=   0.0s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=10, classifier__penalty=l1; total time=   0.0s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=10, classifier__penalty=l1; total time=   0.0s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   1.7s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   1.7s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   1.7s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   1.6s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   2.0s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   1.0s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   1.0s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   0.8s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   1.0s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.001, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   1.1s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   4.1s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l1; total time=   4.1s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   1.9s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   1.4s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   1.4s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   1.1s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   1.2s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   2.1s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   2.3s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   2.7s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   2.0s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.0001, classifier__eta0=0.01, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l2; total time=   2.1s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l1; total time=   4.3s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l1; total time=   4.2s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l1; total time=   4.3s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=10, classifier__penalty=l2; total time=  14.2s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=10, classifier__penalty=l2; total time=  14.6s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=10, classifier__penalty=l2; total time=  15.4s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=0.1, classifier__penalty=l2; total time=   2.3s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l1; total time=   4.7s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=constant, classifier__loss=hinge, classifier__penalty=l1; total time=   4.5s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=1, classifier__penalty=l1; total time=   0.0s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=1, classifier__penalty=l1; total time=   0.0s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=1, classifier__penalty=l1; total time=   0.0s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=1, classifier__penalty=l1; total time=   0.1s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=1, classifier__penalty=l1; total time=   0.1s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=10, classifier__penalty=l2; total time=  16.6s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=10, classifier__penalty=l2; total time=  16.9s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=0.1, classifier__penalty=l2; total time=   2.4s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=0.1, classifier__penalty=l2; total time=   2.4s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   1.2s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=0.1, classifier__penalty=l2; total time=   2.4s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   1.3s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   0.9s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=0.1, classifier__penalty=l2; total time=   3.6s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   1.4s\n",
      "[CV] END classifier=SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42), classifier__alpha=0.01, classifier__eta0=0.001, classifier__learning_rate=optimal, classifier__loss=hinge, classifier__penalty=l2; total time=   1.7s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=1, classifier__penalty=l2; total time=   3.0s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=1, classifier__penalty=l2; total time=   2.9s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=1, classifier__penalty=l2; total time=   3.3s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=1, classifier__penalty=l2; total time=   3.3s\n",
      "[CV] END classifier=LogisticRegression(max_iter=5000, random_state=42), classifier__C=1, classifier__penalty=l2; total time=   3.7s\n",
      "{'classifier__penalty': 'l2', 'classifier__C': 10, 'classifier': LogisticRegression(C=10, max_iter=5000, random_state=42)}\n",
      "CPU times: user 1min 21s, sys: 5.18 s, total: 1min 26s\n",
      "Wall time: 55.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7654929642114626"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = rs.fit(X_train, y_train)\n",
    "print(model.best_params_)\n",
    "model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__penalty</th>\n",
       "      <th>param_classifier__loss</th>\n",
       "      <th>param_classifier__learning_rate</th>\n",
       "      <th>param_classifier__eta0</th>\n",
       "      <th>param_classifier__alpha</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.480827</td>\n",
       "      <td>1.053044</td>\n",
       "      <td>0.052427</td>\n",
       "      <td>0.015897</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(C=10, max_iter=5000, random_state=42)</td>\n",
       "      <td>10</td>\n",
       "      <td>{'classifier__penalty': 'l2', 'classifier__C': 10, 'classifier': LogisticRegression(C=10, max_iter=5000, random_state=42)}</td>\n",
       "      <td>0.758505</td>\n",
       "      <td>0.771273</td>\n",
       "      <td>0.773358</td>\n",
       "      <td>0.762808</td>\n",
       "      <td>0.761520</td>\n",
       "      <td>0.765493</td>\n",
       "      <td>0.005781</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.677389</td>\n",
       "      <td>0.149648</td>\n",
       "      <td>0.022750</td>\n",
       "      <td>0.004974</td>\n",
       "      <td>l2</td>\n",
       "      <td>hinge</td>\n",
       "      <td>optimal</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__penalty': 'l2', 'classifier__loss': 'hinge', 'classifier__learning_rate': 'optimal', 'classifier__eta0': 0.001, 'classifier__alpha': 0.0001, 'classifier': SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)}</td>\n",
       "      <td>0.752486</td>\n",
       "      <td>0.746930</td>\n",
       "      <td>0.739978</td>\n",
       "      <td>0.737325</td>\n",
       "      <td>0.733174</td>\n",
       "      <td>0.741979</td>\n",
       "      <td>0.006901</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.326940</td>\n",
       "      <td>0.266048</td>\n",
       "      <td>0.076025</td>\n",
       "      <td>0.015506</td>\n",
       "      <td>l2</td>\n",
       "      <td>hinge</td>\n",
       "      <td>optimal</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__penalty': 'l2', 'classifier__loss': 'hinge', 'classifier__learning_rate': 'optimal', 'classifier__eta0': 0.01, 'classifier__alpha': 0.0001, 'classifier': SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)}</td>\n",
       "      <td>0.752486</td>\n",
       "      <td>0.746930</td>\n",
       "      <td>0.739978</td>\n",
       "      <td>0.737325</td>\n",
       "      <td>0.733174</td>\n",
       "      <td>0.741979</td>\n",
       "      <td>0.006901</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.122878</td>\n",
       "      <td>0.272319</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>0.086938</td>\n",
       "      <td>l2</td>\n",
       "      <td>hinge</td>\n",
       "      <td>constant</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__penalty': 'l2', 'classifier__loss': 'hinge', 'classifier__learning_rate': 'constant', 'classifier__eta0': 0.01, 'classifier__alpha': 0.0001, 'classifier': SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)}</td>\n",
       "      <td>0.749673</td>\n",
       "      <td>0.745302</td>\n",
       "      <td>0.736478</td>\n",
       "      <td>0.736745</td>\n",
       "      <td>0.735513</td>\n",
       "      <td>0.740742</td>\n",
       "      <td>0.005693</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.576771</td>\n",
       "      <td>0.160482</td>\n",
       "      <td>0.030513</td>\n",
       "      <td>0.009464</td>\n",
       "      <td>l1</td>\n",
       "      <td>hinge</td>\n",
       "      <td>optimal</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__penalty': 'l1', 'classifier__loss': 'hinge', 'classifier__learning_rate': 'optimal', 'classifier__eta0': 0.001, 'classifier__alpha': 0.0001, 'classifier': SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)}</td>\n",
       "      <td>0.736289</td>\n",
       "      <td>0.731990</td>\n",
       "      <td>0.723174</td>\n",
       "      <td>0.726632</td>\n",
       "      <td>0.713230</td>\n",
       "      <td>0.726263</td>\n",
       "      <td>0.007910</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "12      15.480827      1.053044         0.052427        0.015897   \n",
       "5        0.677389      0.149648         0.022750        0.004974   \n",
       "13       1.326940      0.266048         0.076025        0.015506   \n",
       "14       2.122878      0.272319         0.097656        0.086938   \n",
       "4        1.576771      0.160482         0.030513        0.009464   \n",
       "\n",
       "   param_classifier__penalty param_classifier__loss  \\\n",
       "12                        l2                    NaN   \n",
       "5                         l2                  hinge   \n",
       "13                        l2                  hinge   \n",
       "14                        l2                  hinge   \n",
       "4                         l1                  hinge   \n",
       "\n",
       "   param_classifier__learning_rate param_classifier__eta0  \\\n",
       "12                             NaN                    NaN   \n",
       "5                          optimal                  0.001   \n",
       "13                         optimal                   0.01   \n",
       "14                        constant                   0.01   \n",
       "4                          optimal                  0.001   \n",
       "\n",
       "   param_classifier__alpha  \\\n",
       "12                     NaN   \n",
       "5                   0.0001   \n",
       "13                  0.0001   \n",
       "14                  0.0001   \n",
       "4                   0.0001   \n",
       "\n",
       "                                                          param_classifier  \\\n",
       "12                LogisticRegression(C=10, max_iter=5000, random_state=42)   \n",
       "5   SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)   \n",
       "13  SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)   \n",
       "14  SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)   \n",
       "4   SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)   \n",
       "\n",
       "   param_classifier__C  \\\n",
       "12                  10   \n",
       "5                  NaN   \n",
       "13                 NaN   \n",
       "14                 NaN   \n",
       "4                  NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                              params  \\\n",
       "12                                                                                                                        {'classifier__penalty': 'l2', 'classifier__C': 10, 'classifier': LogisticRegression(C=10, max_iter=5000, random_state=42)}   \n",
       "5   {'classifier__penalty': 'l2', 'classifier__loss': 'hinge', 'classifier__learning_rate': 'optimal', 'classifier__eta0': 0.001, 'classifier__alpha': 0.0001, 'classifier': SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)}   \n",
       "13   {'classifier__penalty': 'l2', 'classifier__loss': 'hinge', 'classifier__learning_rate': 'optimal', 'classifier__eta0': 0.01, 'classifier__alpha': 0.0001, 'classifier': SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)}   \n",
       "14  {'classifier__penalty': 'l2', 'classifier__loss': 'hinge', 'classifier__learning_rate': 'constant', 'classifier__eta0': 0.01, 'classifier__alpha': 0.0001, 'classifier': SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)}   \n",
       "4   {'classifier__penalty': 'l1', 'classifier__loss': 'hinge', 'classifier__learning_rate': 'optimal', 'classifier__eta0': 0.001, 'classifier__alpha': 0.0001, 'classifier': SGDClassifier(class_weight='balanced', max_iter=5000, random_state=42)}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "12           0.758505           0.771273           0.773358   \n",
       "5            0.752486           0.746930           0.739978   \n",
       "13           0.752486           0.746930           0.739978   \n",
       "14           0.749673           0.745302           0.736478   \n",
       "4            0.736289           0.731990           0.723174   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "12           0.762808           0.761520         0.765493        0.005781   \n",
       "5            0.737325           0.733174         0.741979        0.006901   \n",
       "13           0.737325           0.733174         0.741979        0.006901   \n",
       "14           0.736745           0.735513         0.740742        0.005693   \n",
       "4            0.726632           0.713230         0.726263        0.007910   \n",
       "\n",
       "    rank_test_score  \n",
       "12                1  \n",
       "5                 2  \n",
       "13                2  \n",
       "14                4  \n",
       "4                 5  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(model.cv_results_).sort_values(by='rank_test_score')\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы**\n",
    "\n",
    "- Было рассмотрено 2 модели (Логистической регрессии и Линейная SVM, использующая стахостический градиентный спуск для оптимизации)\n",
    "- Для каждой модели подобраны гиперпараметры.\n",
    "- В качестве метрики используется F1. Критерий оценки должен быть не ниже 0.75 (условие заказчика) \n",
    "- Смещение модели менее 1.5%, что свидетельствует о том, что модель не переобучена/недообучена.\n",
    "\n",
    "\n",
    "Для тестовой оценки выбрана модель логистической регрессии с гиперпараметрами 'C': 10, 'penalty': 'l2'. В которой текст представляется в векторном виде с использованием tf idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#content'>Вернуться к оглавлению</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='testing'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим точность модели на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика f1 на тестовой выборке 0.78\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"Метрика f1 на тестовой выборке\", f1.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы**\n",
    "\n",
    "- Ошибка на тестовой выборке 0.78, выше нижней требуемой границы.\n",
    "- Модель быстро обучается и делает предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#content'>Вернуться к оглавлению</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "На основании предоставленной информации можно сделать следующие выводы о проделанной работе и полученных результатах:\n",
    "\n",
    "**1. Предобработка данных:**\n",
    "\n",
    "*   Проведена тщательная предобработка текстовых данных, включающая лемматизацию, удаление стоп-слов и дубликатов. Это важный шаг для улучшения качества обучения модели.\n",
    "*   Корпус текстов был очищен от пропусков и дублирующихся индексов.\n",
    "\n",
    "**2. Векторизация текста:**\n",
    "\n",
    "*   Тексты были успешно преобразованы в векторный вид c помощью векторизации TF-IDF. Это позволило использовать тексты в моделях машинного обучения.\n",
    "\n",
    "**4. Обучение моделей:**\n",
    "\n",
    "*   Для решения задачи были обучены модель логистической регрессии и стахостического спуска с функцией потерь модели опорных векторов. С помощью RandomizedSearch и кросс валидации подобраны гиперпараметры моделей.\n",
    "*   \n",
    "\n",
    "**5. Выбор модели:**\n",
    "\n",
    "*   Обоснованно выбрана модель логистической регрессии с гиперпараметрами С: 10 и регуляризацией l2, обученая на признаках TF-IDF.\n",
    "\n",
    "**6. Тестирование модели:**\n",
    "\n",
    "*   На тестовой выборке модель показала результат F1-меры 0.78, что превышает минимально требуемый порог 0.75. Это свидетельствует о хорошей обобщающей способности модели.\n",
    "\n",
    "**7. Производительность модели:**\n",
    "\n",
    "*   Отмечено быстрое обучение модели и высокая скорость предсказания, что является важным фактором для практического применения.\n",
    "\n",
    "**Общий вывод:**\n",
    "\n",
    "Получена модель классификации текстов, удовлетворяющая требованиям заказчика по качеству (F1-мера выше 0.75) и производительности. \n",
    "\n",
    "**Рекомендации:**\n",
    "\n",
    "*   Несмотря на хорошие результаты, всегда есть возможность для улучшения. Можно попробовать использовать другие методы векторизации текста (например, BERT) и сравнить результаты.\n",
    "*   Для дальнейшего улучшения модели можно рассмотреть возможность использования более сложных моделей классификации, таких как градиентный бустинг.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#content'>Вернуться к оглавлению</a> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "234px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
